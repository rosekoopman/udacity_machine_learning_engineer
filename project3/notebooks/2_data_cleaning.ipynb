{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handy-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "actual-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    \n",
    "    # initiate dict to store data\n",
    "    dfs = {}\n",
    "\n",
    "    for f in files:\n",
    "\n",
    "        # get name of the water body from the filename\n",
    "        database = f.split('\\\\')[-1].replace('.csv','')\n",
    "\n",
    "        # load the data\n",
    "        df = pd.read_csv(f)\n",
    "\n",
    "        # drop rows for which all values are NaN\n",
    "        df.dropna(how='all', axis=0, inplace=True)\n",
    "\n",
    "        # get start and end data of the data\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "        # store data in dictionary by water body name\n",
    "        dfs[database] = df\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "flexible-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dfclean, date_min, date_max, drop_cols=[], replace_cols=[], rename_cols={}, offsets=[]):\n",
    "    \n",
    "    \n",
    "    # select time range and make sure there is a record for each day\n",
    "    dfclean = pd.merge(pd.DataFrame(index=pd.date_range(date_min, date_max)),\n",
    "                       dfclean.set_index('Date'),\n",
    "                       left_index=True,\n",
    "                       right_index=True,\n",
    "                       how='left')\n",
    "    \n",
    "    # rename columns\n",
    "    dfclean.rename(columns=rename_cols, inplace=True)\n",
    "    \n",
    "    # drop columns\n",
    "    dfclean.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "    # set dtype\n",
    "    for c in dfclean.columns:\n",
    "        dfclean[c] = dfclean[c].astype(float)\n",
    "\n",
    "    # fix data offsets\n",
    "    for col, offset, loc_min, loc_max in offsets:\n",
    "        dfclean.loc[loc_min:loc_max, col] = dfclean.loc[loc_min:loc_max, col] + offset\n",
    "        \n",
    "    # replace invalid values by NaN\n",
    "    for c, values in replace_cols:\n",
    "        dfclean[c] = dfclean[c].replace(*values)\n",
    "\n",
    "    # interpolate data (if only 1 subsequent value is missing)\n",
    "    for c in dfclean.columns:\n",
    "        dfclean[c] = dfclean[c].interpolate(method='time', limit=1)\n",
    "        \n",
    "    # all flow columns must have positive values\n",
    "    for c in dfclean.filter(regex='^Flow_Rate').columns:\n",
    "        dfclean[c] = dfclean[c].abs()\n",
    "        \n",
    "    dfclean.reset_index(inplace=True)\n",
    "    dfclean.rename(columns={'index':'Date'}, inplace=True)\n",
    "        \n",
    "    print('shape', dfclean.shape)\n",
    "        \n",
    "    return dfclean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-chancellor",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "inclusive-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '../data/'\n",
    "rawdir = os.path.join(datadir, 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rising-confidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join(rawdir, '*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abstract-employee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = load_data(files)\n",
    "names = list(dfs.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-facing",
   "metadata": {},
   "source": [
    "# Clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-zoning",
   "metadata": {},
   "source": [
    "## Auser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "norman-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "nr=0\n",
    "\n",
    "name = names[nr]\n",
    "filename = files[nr].replace('raw','clean')\n",
    "\n",
    "# time selection\n",
    "date_min = pd.to_datetime('2015-01-01')\n",
    "date_max = pd.to_datetime('2020-06-30')\n",
    "\n",
    "# feature selection based on availability of data\n",
    "drop_cols = ['Temperature_Ponte_a_Moriano']\n",
    "\n",
    "# replace invalid values with NaN\n",
    "replace_cols = [('Depth_to_Groundwater_COS', (0, np.nan)), ('Depth_to_Groundwater_SAL', (0, np.nan))]\n",
    "\n",
    "# rename columns\n",
    "rename_cols = {'Depth_to_Groundwater_CoS': 'Depth_to_Groundwater_COS'}\n",
    "\n",
    "# offset\n",
    "offsets = [('Hydrometry_Piaggione', 1.19, '2010-01-01', '2021-01-01')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "secure-cambridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (2008, 26)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "dfclean = clean_data(dfs[name].copy(), \n",
    "                     date_min, \n",
    "                     date_max, \n",
    "                     drop_cols=drop_cols, \n",
    "                     replace_cols=replace_cols, \n",
    "                     rename_cols=rename_cols, \n",
    "                     offsets=offsets)\n",
    "\n",
    "# save data\n",
    "dfclean.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "entertaining-strength",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                                         0\n",
       "Rainfall_Gallicano                           0\n",
       "Rainfall_Pontetetto                          0\n",
       "Rainfall_Monte_Serra                         0\n",
       "Rainfall_Orentano                            0\n",
       "Rainfall_Borgo_a_Mozzano                     0\n",
       "Rainfall_Piaggione                           0\n",
       "Rainfall_Calavorno                           0\n",
       "Rainfall_Croce_Arcana                        0\n",
       "Rainfall_Tereglio_Coreglia_Antelminelli      0\n",
       "Rainfall_Fabbriche_di_Vallico                0\n",
       "Depth_to_Groundwater_LT2                   184\n",
       "Depth_to_Groundwater_SAL                   114\n",
       "Depth_to_Groundwater_PAG                    12\n",
       "Depth_to_Groundwater_COS                    80\n",
       "Depth_to_Groundwater_DIEC                  135\n",
       "Temperature_Orentano                         0\n",
       "Temperature_Monte_Serra                      0\n",
       "Temperature_Lucca_Orto_Botanico              0\n",
       "Volume_POL                                   0\n",
       "Volume_CC1                                   0\n",
       "Volume_CC2                                   0\n",
       "Volume_CSA                                   0\n",
       "Volume_CSAL                                  0\n",
       "Hydrometry_Monte_S_Quirico                   0\n",
       "Hydrometry_Piaggione                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: there are still missing values in the Depth_to_Groundwater_* columns. \n",
    "#       This will be dealt with later, after creation of lagged features\n",
    "dfclean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-musical",
   "metadata": {},
   "source": [
    "## Petrignano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "driven-feedback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "nr=3\n",
    "\n",
    "name = names[nr]\n",
    "filename = files[nr].replace('raw','clean')\n",
    "\n",
    "# time selection\n",
    "date_min = pd.to_datetime('2016-01-01')\n",
    "date_max = pd.to_datetime('2020-06-30')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "alternate-format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (1643, 8)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "dfclean = clean_data(dfs[name].copy(), date_min, date_max)\n",
    "\n",
    "# save data\n",
    "dfclean.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "composite-baseball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                                    0\n",
       "Rainfall_Bastia_Umbra                   0\n",
       "Depth_to_Groundwater_P24                9\n",
       "Depth_to_Groundwater_P25                9\n",
       "Temperature_Bastia_Umbra                0\n",
       "Temperature_Petrignano                  0\n",
       "Volume_C10_Petrignano                   0\n",
       "Hydrometry_Fiume_Chiascio_Petrignano    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: there are still missing values in the Depth_to_Groundwater_* columns. \n",
    "#       This will be dealt with later, after creation of lagged features\n",
    "dfclean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-idaho",
   "metadata": {},
   "source": [
    "## River Arno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "saved-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "nr=5\n",
    "\n",
    "name = names[nr]\n",
    "filename = files[nr].replace('raw','clean')\n",
    "\n",
    "# time selection\n",
    "date_min = pd.to_datetime('2004-01-01')\n",
    "date_max = pd.to_datetime('2020-06-30')\n",
    "\n",
    "# feature selection based on availability of data\n",
    "drop_cols = ['Rainfall_Vernio','Rainfall_Stia', 'Rainfall_Consuma', 'Rainfall_Incisa',\n",
    "             'Rainfall_Montevarchi', 'Rainfall_S_Savino', 'Rainfall_Laterina',\n",
    "             'Rainfall_Bibbiena', 'Rainfall_Camaldoli', 'Temperature_Firenze']\n",
    "\n",
    "# replace invalid value by NaN\n",
    "replace_cols = [('Hydrometry_Nave_di_Rosano', (0, np.nan))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "instrumental-chorus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (6026, 7)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "dfclean = clean_data(dfs[name].copy(), date_min, date_max, drop_cols=drop_cols, replace_cols=replace_cols)\n",
    "\n",
    "# save data\n",
    "dfclean.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "underlying-schedule",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                           0\n",
       "Rainfall_Le_Croci              0\n",
       "Rainfall_Cavallina             0\n",
       "Rainfall_S_Agata               0\n",
       "Rainfall_Mangona               0\n",
       "Rainfall_S_Piero               0\n",
       "Hydrometry_Nave_di_Rosano    183\n",
       "dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: there are still missing values for Hydrometry_Navi_di_Rosano. \n",
    "#       This will be dealt with later, after creation of lagged features\n",
    "dfclean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-server",
   "metadata": {},
   "source": [
    "## Lake Bilancino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "composed-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "nr=4\n",
    "\n",
    "name = names[nr]\n",
    "filename = files[nr].replace('raw','clean')\n",
    "\n",
    "# time selection\n",
    "date_min = pd.to_datetime('2004-01-01')\n",
    "date_max = pd.to_datetime('2020-06-30')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "grateful-smell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape (6026, 9)\n"
     ]
    }
   ],
   "source": [
    "# clean data\n",
    "dfclean = clean_data(dfs[name].copy(), date_min, date_max)\n",
    "\n",
    "# save data\n",
    "dfclean.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-wallet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-verse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
